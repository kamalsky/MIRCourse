{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMPLab assignment: text-based audio classification \n",
    "Original notebook wirtten by Frederic Font, extended by Philip Tovstogan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART B - Classification\n",
    "\n",
    "To run the cells in this notebook you'll need to install the following Python dependencies:\n",
    " * `numpy`\n",
    " * `sklearn`\n",
    " * `matplotlib`\n",
    "\n",
    "If you have not installed them you should be able to do so by running: `pip install numpy sklearn matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import utils\n",
    "import random\n",
    "import collections\n",
    "import numpy\n",
    "from sklearn import svm, tree\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "try:  # This is for compatiblitily python2/python3 of xrange function\n",
    "    xrange\n",
    "except NameError:\n",
    "    xrange = range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load previously created dataset\n",
    "\n",
    "Set `DATASET_NAME` to the dataset you want to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 2 classes:\n",
      "\tPlucked string instrument: 200 sounds\n",
      "\tBowed string instrument: 200 sounds\n"
     ]
    }
   ],
   "source": [
    "# Load dataset from saved file\n",
    "DATASET_NAME = 'bowed_vs_plucked'\n",
    "dataset = utils.load_from_json('%s.json' % DATASET_NAME)\n",
    "print('Loaded dataset with %i classes:' % len(dataset))\n",
    "for klass, sounds in dataset.items():\n",
    "    print('\\t%s: %i sounds' % (klass, len(sounds)))\n",
    "class_names = list(dataset.keys())  # This is just for convenience, reused later on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Define vector space\n",
    "\n",
    "Here we create a prototype feature vector that will define the way in which our documetns are represented for classification purposes. You can set the number of dimensions of the vector sepace/feature vector by editing the parameter `NUMBER_OF_DIMENSIONS_OF_FEATURE_VECTOR` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created prototype feature vector with 5 dimensions (originally the space had 430 dimensions)\n",
      "Prototype vector tags (sorted by occurrence in filtered_tags):\n",
      "\t1 multisample (146 ocurrences)\n",
      "\t2 guitar (135 ocurrences)\n",
      "\t3 acoustic (97 ocurrences)\n",
      "\t4 single-note (96 ocurrences)\n",
      "\t5 pizzicato (81 ocurrences)\n"
     ]
    }
   ],
   "source": [
    "def build_tag_vector_space(n_dimensions, dataset, class_names): \n",
    "    # Get all tags in the dataset (the vocabulary)\n",
    "    all_tags = list()\n",
    "    for class_name in class_names:\n",
    "        class_tags = utils.get_all_tags_from_class(class_name, dataset)\n",
    "        all_tags += class_tags\n",
    "        \n",
    "    # Filter out tags with less frequency (get only top N tags)\n",
    "    most_common_tags_counts = collections.Counter(all_tags).most_common(n_dimensions)\n",
    "    most_common_tags = [tag for tag, _ in most_common_tags_counts]\n",
    "    filtered_tags = [tag for tag in most_common_tags if tag in all_tags]\n",
    "    \n",
    "    # Build our prototype feature vector (unique list of tags), and print first 10 tags\n",
    "    prototype_feature_vector = list(set(filtered_tags))\n",
    "    print('Created prototype feature vector with %i dimensions (originally the space had %i dimensions)' % (\n",
    "        len(prototype_feature_vector), len(set(all_tags))))\n",
    "    print('Prototype vector tags (sorted by occurrence in filtered_tags):')\n",
    "    for count, (tag, frequency) in enumerate(most_common_tags_counts):\n",
    "        print('\\t%i %s (%i ocurrences)' % (count + 1, tag, frequency))\n",
    "    return prototype_feature_vector\n",
    "\n",
    "\n",
    "NUMBER_OF_DIMENSIONS_OF_FEATURE_VECTOR = 5 # Maximum number of dimensions for the feature vector.\n",
    "\n",
    "prototype_feature_vector = build_tag_vector_space(\n",
    "    n_dimensions=NUMBER_OF_DIMENSIONS_OF_FEATURE_VECTOR,\n",
    "    dataset=dataset,\n",
    "    class_names=class_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Project documents in the vector space\n",
    "\n",
    "The cell below shows you how to project a document to the vector space, that is to say, how to get the feature vector of a specific sound form our dataset. You can run this cell multiple times to see the feature vector of different randomly chosen sounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe frameborder=\"0\" scrolling=\"no\" src=\"http://www.freesound.org/embed/sound/iframe/169945/simple/medium/\" width=\"481\" height=\"86\"></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen sound has tags: jangle, toy, string, ukulele, children, jingle, plastic, guitar, instrument, childs, thrum, strum, child, acoustic, uke, tinkle, strings, chink, kid\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td>multisample</td><td>guitar</td><td>pizzicato</td><td>single-note</td><td>acoustic</td></tr><tr><td></td><td>x</td><td></td><td></td><td>x</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of getting feature vector from tags list...\n",
    "random_sound = random.choice(dataset[random.choice(class_names)])\n",
    "random_sound_tags = random_sound['tags']\n",
    "random_sound_feature_vector = utils.get_feature_vector_from_tags(random_sound_tags, prototype_feature_vector)\n",
    "\n",
    "display(HTML(utils.get_sound_embed_html(random_sound['id'])))\n",
    "print('Chosen sound has tags:', ', '.join(random_sound_tags))\n",
    "html = '<table><tr><td>'\n",
    "html += '</td><td>'.join([tag for tag in prototype_feature_vector])\n",
    "html += '</td></tr><tr><td>'\n",
    "html += '</td><td>'.join(['x' if prototype_feature_vector[count] in random_sound_tags else '' \n",
    "                          for count, item in enumerate(random_sound_feature_vector)])\n",
    "html += '</td></tr></table>'\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Define train and testing set\n",
    "\n",
    "In this cell we create the training and test sets that will be used to train our classifier and evaluate its accuracy.\n",
    "\n",
    "Set the `PERCENTAGE_OF_TRAINING_DATA` to decide which percentage of data goes to training and which goes to testing.\n",
    "Set `MAX_INPUT_TAGS_FOR_TESTING` to decide the maximum number of tags that will be used for each sound in the test send to predict its category. Set it to a big number (~20) to effectievly bypass this parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created training and testing sets with the following number of sounds:\n",
      "\tTrain\tTest\n",
      "\t150\t50\tPlucked string instrument\n",
      "\t150\t50\tBowed string instrument\n"
     ]
    }
   ],
   "source": [
    "def create_train_and_test_sets(dataset, class_names, percentage_training_data, \n",
    "                               max_input_tags_for_testing):\n",
    "    training_set = dict()\n",
    "    testing_set = dict()\n",
    "\n",
    "    # Get 'n_training_sounds_per_class' sounds per class \n",
    "    for class_name, sounds in dataset.items():\n",
    "        n_training_sounds_per_class = int(len(sounds) * percentage_training_data)\n",
    "        sounds_from_class = sounds[:] # Copy the list so when we later shuffle it does not affect the original data \n",
    "        random.shuffle(sounds_from_class)\n",
    "        training_set[class_name] = sounds_from_class[:n_training_sounds_per_class] # First sounds for training\n",
    "        testing_set[class_name] = sounds_from_class[n_training_sounds_per_class:] # Following sounds for testing\n",
    "     \n",
    "        # Save a trimmed version of input tags for testing sounds\n",
    "        for sound in testing_set[class_name]:\n",
    "            sound['tags'] = random.sample(sound['tags'], min(max_input_tags_for_testing, len(sound['tags'])))\n",
    "\n",
    "    print('Created training and testing sets with the following number of sounds:\\n\\tTrain\\tTest')\n",
    "    for class_name in class_names:\n",
    "        training_sounds = training_set[class_name]\n",
    "        testing_sounds = testing_set[class_name]\n",
    "        print('\\t%i\\t%i\\t%s' % (len(training_sounds), len(testing_sounds), class_name))\n",
    "    return training_set, testing_set\n",
    "\n",
    "\n",
    "PERCENTAGE_OF_TRAINING_DATA = 0.75 # Percentage of sounds that will be used for training (others are for testing)\n",
    "MAX_INPUT_TAGS_FOR_TESTING = 20 # Use a big number to \"omit\" this parameter and use as many tags as originally are in the sound\n",
    "\n",
    "training_set, testing_set = create_train_and_test_sets(\n",
    "    dataset=dataset, \n",
    "    class_names=class_names,\n",
    "    percentage_training_data=PERCENTAGE_OF_TRAINING_DATA,\n",
    "    max_input_tags_for_testing=MAX_INPUT_TAGS_FOR_TESTING,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Train classifier\n",
    "\n",
    "Train the classifier with the training set that we prepared. User `CLASSIFIER_TYPE` parameter below to chose which type of classifier you want to use. This code currently supports Super Vector Mahcines (`svm`), and Decision Trees (`tree`). You might want to try adding further classifier types here.\n",
    "\n",
    "Note that when using the `tree` classifier, the output of the tree is saved into an image and shown here. This is interesting to learn about what did the classifier learn. To show these trees, you'll need to install **Graphviz** and run the `dot` command line tool. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier (tree) with 300 sounds...\n",
      "if guitar =< 0.5: \n",
      "  |then if single-note =< 0.5: \n",
      "  |  |then if pizzicato =< 0.5: \n",
      "  |  |  |then if multisample =< 0.5: \n",
      "  |  |  |  |then if acoustic =< 0.5: \n",
      "  |  |  |  |  |then Plucked string instrument\n",
      "  |  |  |  |  |else Bowed string instrument\n",
      "  |  |  |  |else Bowed string instrument\n",
      "  |  |  |else Bowed string instrument\n",
      "  |  |else Bowed string instrument\n",
      "  |else Plucked string instrument\n",
      "<------------->\n",
      "Tree Depth:  5\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "def build_tag_feature_vector(sound):\n",
    "    tag_features = utils.get_feature_vector_from_tags(sound['tags'], prototype_feature_vector)\n",
    "    return np.concatenate([[], tag_features])\n",
    "\n",
    "def train_classifier(training_set, classifier_type, class_names, dataset_name, feature_vector_func, \n",
    "                     feature_vector_dimension_labels=None, tree_max_depth=5):\n",
    "    \n",
    "    # Prepare data for fitting classifier (as sklearn classifiers require)\n",
    "    classes_vector = list()\n",
    "    feature_vectors = list()\n",
    "    for class_name, sounds in training_set.items():\n",
    "        for count, sound in enumerate(sounds):\n",
    "            # Use index of class name in class_names as numerical value (classifier internally represents \n",
    "            # class label as number)\n",
    "            classes_vector.append(class_names.index(class_name))\n",
    "            feature_vector = feature_vector_func(sound)\n",
    "            feature_vectors.append(feature_vector)\n",
    "\n",
    "    # Create and fit classifier\n",
    "    print('Training classifier (%s) with %i sounds...' % (CLASSIFIER_TYPE, len(feature_vectors)))\n",
    "    if classifier_type == 'svm':\n",
    "        classifier = svm.LinearSVC()\n",
    "        classifier.fit(feature_vectors, classes_vector)\n",
    "    elif classifier_type == 'tree':\n",
    "        classifier = tree.DecisionTreeClassifier(max_depth=tree_max_depth)\n",
    "        classifier.fit(feature_vectors, classes_vector)\n",
    "        \n",
    "        # Plot classifier decision rules\n",
    "        utils.print_tree_as_text(classifier, feature_vector_dimension_labels, class_names)\n",
    "    \n",
    "        # Alternatively you can do nicer plot with images, but this requires Graphviz to be installed\n",
    "        # WARNING: do not run this if tree is too big, might freeze\n",
    "        #out_filename = '%s_tree_%i.png' % (dataset_name, random.randint(1000,9999))\n",
    "        #utils.export_tree_as_graph(\n",
    "        #    classifier, feature_vector_dimension_labels, class_names=class_names, filename=out_filename)\n",
    "        #display(HTML('<h4>Learned tree:</h4><img src=\"%s\"/>' % out_filename))\n",
    "    else:\n",
    "        raise Exception('Bad classifier type!!!')\n",
    "    \n",
    "    print('done!')\n",
    "    return classifier\n",
    "\n",
    "CLASSIFIER_TYPE = 'tree' # Use 'svm' or 'tree'\n",
    "\n",
    "classifier = train_classifier(\n",
    "    training_set=training_set,\n",
    "    classifier_type=CLASSIFIER_TYPE, \n",
    "    class_names=class_names, \n",
    "    dataset_name=DATASET_NAME,\n",
    "    feature_vector_func=build_tag_feature_vector,\n",
    "    feature_vector_dimension_labels=prototype_feature_vector,  # This is used to show the class names in the tree image\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Evaluate classification\n",
    "\n",
    "The function below evaluated the classifier build in the previous cell and shows some results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating with 100 instances...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of features of the model must match the input. Model n_features is 7 and input n_features is 5 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-bfe4cd2899c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mclassifier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mclass_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mfeature_vector_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_tag_feature_vector\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-92-bfe4cd2899c0>\u001b[0m in \u001b[0;36mevaluate_classifier\u001b[0;34m(testing_set, classifier, class_names, feature_vector_func, show_confusion_matrix)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msound\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msounds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mfeature_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_vector_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mpredicted_class_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_vector\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0mpredicted_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msound\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_class_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'done!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    410\u001b[0m         \"\"\"\n\u001b[1;32m    411\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tree_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m         \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    382\u001b[0m                              \u001b[0;34m\"match the input. Model n_features is %s and \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m                              \u001b[0;34m\"input n_features is %s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m                              % (self.n_features_, n_features))\n\u001b[0m\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Number of features of the model must match the input. Model n_features is 7 and input n_features is 5 "
     ]
    }
   ],
   "source": [
    "def evaluate_classifier(testing_set, classifier, class_names, feature_vector_func, show_confusion_matrix=True):\n",
    "    # Test with testing set\n",
    "    print('Evaluating with %i instances...' % sum([len(sounds) for sounds in testing_set.values()]))\n",
    "    predicted_data = list()\n",
    "    for class_name, sounds in testing_set.items():\n",
    "        for count, sound in enumerate(sounds):\n",
    "            feature_vector = feature_vector_func(sound)\n",
    "            predicted_class_name = class_names[classifier.predict([feature_vector])[0]]\n",
    "            predicted_data.append((sound['id'], class_name, predicted_class_name))     \n",
    "    print('done!')\n",
    "\n",
    "    # Compute overall accuracy\n",
    "    good_predictions = len([1 for sid, cname, pname in predicted_data if cname == pname])\n",
    "    wrong_predictions = len([1 for sid, cname, pname in predicted_data if cname != pname])\n",
    "    print('%i correct predictions' % good_predictions)\n",
    "    print('%i wrong predictions' % wrong_predictions)\n",
    "    accuracy = float(good_predictions)/(good_predictions + wrong_predictions)\n",
    "    print('Overall accuracy %.2f%%' % (100 * accuracy))\n",
    "    \n",
    "    if show_confussing_matrix:\n",
    "        # Compute confussion matrix (further analysis)\n",
    "        matrix = list()\n",
    "        for class_name in class_names:\n",
    "            predicted_classes = list()\n",
    "            for sid, cname, pname in predicted_data:\n",
    "                if cname == class_name:\n",
    "                    predicted_classes.append(pname)\n",
    "            matrix.append([predicted_classes.count(target_class) for target_class in class_names])\n",
    "\n",
    "        # Plot confussion matrix\n",
    "        fig = plt.figure()\n",
    "        plt.clf()\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.set_aspect(1)\n",
    "        res = ax.imshow(matrix, cmap=plt.cm.Blues, interpolation='nearest')\n",
    "\n",
    "        for x in xrange(len(matrix)):\n",
    "            for y in xrange(len(matrix)):\n",
    "                ax.annotate(str(matrix[x][y]), xy=(y, x), \n",
    "                            horizontalalignment='center',\n",
    "                            verticalalignment='center')\n",
    "\n",
    "        shortened_class_names = [item[0:10] for item in class_names]\n",
    "        plt.xticks(range(len(class_names)), shortened_class_names, rotation=90)\n",
    "        plt.yticks(range(len(class_names)), shortened_class_names)\n",
    "        plt.xlabel('Predicted classes')\n",
    "        plt.ylabel('Groundtruth classes')\n",
    "\n",
    "        print('Confussion matrix')\n",
    "        plt.show()\n",
    "    \n",
    "    return accuracy\n",
    "    \n",
    "evaluate_classifier(\n",
    "    testing_set=testing_set,\n",
    "    classifier=classifier,\n",
    "    class_names=class_names,\n",
    "    feature_vector_func=build_tag_feature_vector,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#AB4646;\">Using audio features</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created prototype feature vector with 7 dimensions (originally the space had 318 dimensions)\n",
      "Prototype vector tags (sorted by occurrence in filtered_tags):\n",
      "\t1 guitar (126 ocurrences)\n",
      "\t2 multisample (116 ocurrences)\n",
      "\t3 acoustic (87 ocurrences)\n",
      "\t4 single-note (79 ocurrences)\n",
      "\t5 pizzicato (65 ocurrences)\n",
      "\t6 violin (61 ocurrences)\n",
      "\t7 cello (57 ocurrences)\n",
      "Created training and testing sets with the following number of sounds:\n",
      "\tTrain\tTest\n",
      "\t150\t50\tPlucked string instrument\n",
      "\t150\t50\tBowed string instrument\n",
      "Training classifier (tree) with 300 sounds...\n",
      "if guitar =< 0.5: \n",
      "  |then if multisample =< 0.5: \n",
      "  |  |then if violin =< 0.5: \n",
      "  |  |  |then if cello =< 0.5: \n",
      "  |  |  |  |then if hfc =< 0.9978755712509155: \n",
      "  |  |  |  |  |then Bowed string instrument\n",
      "  |  |  |  |  |else Plucked string instrument\n",
      "  |  |  |  |else Bowed string instrument\n",
      "  |  |  |else Bowed string instrument\n",
      "  |  |else if single-note =< 0.5: \n",
      "  |  |  |then if hfc =< 1.4460893869400024: \n",
      "  |  |  |  |then Bowed string instrument\n",
      "  |  |  |  |else if hfc =< 3.35610294342041: \n",
      "  |  |  |  |  |then Plucked string instrument\n",
      "  |  |  |  |  |else Bowed string instrument\n",
      "  |  |  |else Bowed string instrument\n",
      "  |else Plucked string instrument\n",
      "<------------->\n",
      "Tree Depth:  5\n",
      "done!\n",
      "Evaluating with 100 instances...\n",
      "done!\n",
      "92 correct predictions\n",
      "8 wrong predictions\n",
      "Overall accuracy 92.00%\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'show_confussing_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-bcbbd62f572c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m evaluate_classifier(\n\u001b[1;32m     47\u001b[0m     \u001b[0mtesting_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtesting_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     feature_vector_func=build_feature_vector)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-92-bfe4cd2899c0>\u001b[0m in \u001b[0;36mevaluate_classifier\u001b[0;34m(testing_set, classifier, class_names, feature_vector_func, show_confusion_matrix)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Overall accuracy %.2f%%'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mshow_confussing_matrix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Compute confussion matrix (further analysis)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mmatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'show_confussing_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "# Expriment using duration\n",
    "\n",
    "CLASSIFIER_TYPE = 'tree'\n",
    "PERCENTAGE_OF_TRAINING_DATA = 0.75\n",
    "NUMBER_OF_DIMENSIONS_OF_FEATURE_VECTOR = 5\n",
    "MAX_INPUT_TAGS_FOR_TESTING = 20\n",
    "\n",
    "def build_feature_vector(sound):\n",
    "    tag_features = utils.get_feature_vector_from_tags(sound['tags'], prototype_feature_vector)\n",
    "    audio_features = [\n",
    "        sound['analysis']['lowlevel']['spectral_centroid']['mean'],\n",
    "        sound['analysis']['lowlevel']['hfc']['mean']\n",
    "    ]\n",
    "    # Some sounds seem to have no duration value (no analysis), in this case we sould probably remove them from the dataset\n",
    "    return np.concatenate([[], tag_features, numpy.array(audio_features)])\n",
    "\n",
    "# CODE TO CLEAN DATASET (remove sounds which do not have analysis information)\n",
    "dataset_cleaned = {}\n",
    "for key, sounds in dataset.items():\n",
    "    if key not in dataset_cleaned:\n",
    "        dataset_cleaned[key] = []\n",
    "    for sound in sounds:\n",
    "        if sound['analysis'] is not None:\n",
    "            dataset_cleaned[key].append(sound)\n",
    "\n",
    "prototype_feature_vector = []\n",
    "prototype_feature_vector = build_tag_vector_space(\n",
    "    n_dimensions=NUMBER_OF_DIMENSIONS_OF_FEATURE_VECTOR,\n",
    "    dataset=dataset,\n",
    "    class_names=class_names,\n",
    ")\n",
    "\n",
    "training_set, testing_set = create_train_and_test_sets(\n",
    "    dataset=dataset_cleaned, class_names=class_names,\n",
    "    percentage_training_data=PERCENTAGE_OF_TRAINING_DATA, max_input_tags_for_testing=MAX_INPUT_TAGS_FOR_TESTING)\n",
    "\n",
    "feature_names = prototype_feature_vector[:]\n",
    "feature_names.append('spectral_centroid')\n",
    "feature_names.append('hfc')\n",
    "\n",
    "classifier = train_classifier(\n",
    "    training_set=training_set, classifier_type=CLASSIFIER_TYPE, class_names=class_names, dataset_name=DATASET_NAME,\n",
    "    feature_vector_func=build_feature_vector, feature_vector_dimension_labels=feature_names,\n",
    ")\n",
    "\n",
    "evaluate_classifier(\n",
    "    testing_set=testing_set, classifier=classifier, class_names=class_names, \n",
    "    feature_vector_func=build_feature_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created training and testing sets with the following number of sounds:\n",
      "\tTrain\tTest\n",
      "\t150\t50\tPlucked string instrument\n",
      "\t150\t50\tBowed string instrument\n",
      "Training classifier (svm) with 300 sounds...\n",
      "done!\n",
      "Training classifier (svm) with 300 sounds...\n",
      "done!\n",
      "Evaluating with 100 instances...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 9 features per sample; expecting 7",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-87e8c353a357>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m tag_accuracy = evaluate_classifier(\n\u001b[1;32m     21\u001b[0m     \u001b[0mtesting_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtesting_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtag_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     feature_vector_func=build_feature_vector, show_confusion_matrix=False)\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m audio_accuracy = evaluate_classifier(\n",
      "\u001b[0;32m<ipython-input-92-bfe4cd2899c0>\u001b[0m in \u001b[0;36mevaluate_classifier\u001b[0;34m(testing_set, classifier, class_names, feature_vector_func, show_confusion_matrix)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msound\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msounds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mfeature_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_vector_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mpredicted_class_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_vector\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0mpredicted_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msound\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_class_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'done!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \"\"\"\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[0;32m--> 305\u001b[0;31m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
      "\u001b[0;31mValueError\u001b[0m: X has 9 features per sample; expecting 7"
     ]
    }
   ],
   "source": [
    "CLASSIFIER_TYPE = 'tree'\n",
    "PERCENTAGE_OF_TRAINING_DATA = 0.75\n",
    "NUMBER_OF_DIMENSIONS_OF_FEATURE_VECTOR = 7\n",
    "MAX_INPUT_TAGS_FOR_TESTING = 20\n",
    "\n",
    "training_set, testing_set = create_train_and_test_sets(\n",
    "    dataset=dataset_cleaned, class_names=class_names,\n",
    "    percentage_training_data=PERCENTAGE_OF_TRAINING_DATA, max_input_tags_for_testing=MAX_INPUT_TAGS_FOR_TESTING)\n",
    "\n",
    "audio_classifier = train_classifier(\n",
    "    training_set=training_set, classifier_type=CLASSIFIER_TYPE, class_names=class_names, dataset_name=DATASET_NAME,\n",
    "    feature_vector_func=build_feature_vector, feature_vector_dimension_labels=feature_names,\n",
    ")\n",
    "\n",
    "tag_classifier = train_classifier(\n",
    "    training_set=training_set, classifier_type=CLASSIFIER_TYPE, class_names=class_names, dataset_name=DATASET_NAME,\n",
    "    feature_vector_func=build_tag_feature_vector, feature_vector_dimension_labels=prototype_feature_vector, \n",
    ")\n",
    "\n",
    "tag_accuracy = evaluate_classifier(\n",
    "    testing_set=testing_set, classifier=tag_classifier, class_names=class_names, \n",
    "    feature_vector_func=build_feature_vector, show_confusion_matrix=False)\n",
    "\n",
    "audio_accuracy = evaluate_classifier(\n",
    "    testing_set=testing_set, classifier=audio_classifier, class_names=class_names, \n",
    "    feature_vector_func=build_feature_vector, show_confusion_matrix=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
